{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "occupied-attendance",
   "metadata": {},
   "source": [
    "# 2. Text Preprocessing\n",
    "The objective of this notebook is to process the article data and prepare that for modeling. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "developmental-recording",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Typically, any NLP-based problem can be solved by a methodical workflow that has a sequence of steps. The major steps are depicted in the following figure.\n",
    "\n",
    "[1]: https://towardsdatascience.com/a-practitioners-guide-to-natural-language-processing-part-i-processing-understanding-text-9f4abfd13e72\n",
    "\n",
    "<div>\n",
    "<img src=\"./img/NLP-workflow.png\" width=\"650\"/>\n",
    "</div>\n",
    "\n",
    "We usually start with a corpus of text documents and follow standard processes of text wrangling and pre-processing, parsing and basic exploratory data analysis. Based on the initial insights, we usually represent the text using relevant feature engineering techniques. Depending on the problem at hand, we either focus on building predictive supervised models or unsupervised models, which usually focus more on pattern mining and grouping. Finally, we evaluate the model and the overall success criteria with relevant stakeholders or customers, and deploy the final model for future usage."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "967f48e0",
   "metadata": {},
   "source": [
    "Text produced through web scraping is typically highly noisy containing spelling errors, abbreviations, non-standard words, false starts, repetitions, missing punctuations, missing letter case information, pause filling words such as “um” and “uh” and other texting and speech disfluencies. Such text can be seen in large amounts in contact centers, chat rooms, optical character recognition (OCR) of text documents, short message service (SMS) text, etc. Generally the pre-processing steps to approach any NLP problems include:\n",
    "\n",
    "1. **Noise Cleaning**: Noise is a common issue in unstructured text. Noisy unstructured text data is found in informal settings such as online chat, text messages, e-mails, message boards, newsgroups, blogs, wikis and web pages. Also, text produced by processing spontaneous speech using automatic speech recognition and printed or handwritten text using optical character recognition contains processing noise. Noise removal usually consist removal of HTML tags, white spaces, punctuations, etc.\n",
    "\n",
    "2. **Spell Checking**: In this step the misspelled words are fixed, accented characters are replaced with appropriate english characters, and strings with no meaning are filtered.\n",
    "\n",
    "3. **Contraction Mapping**: Contractions are words or combinations of words that are shortened by dropping letters and replacing them by an apostrophe. In English contractions, we often drop the vowels from a word to form the contractions. Removing contractions contributes to text standardization. \n",
    "\n",
    "4. **Stemming/Lemmatization**: Stemming usually refers to a crude heuristic process that chops off the ends of words in the hope of achieving this goal correctly most of the time, and often includes the removal of derivational affixes. Lemmatization usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma .\n",
    "\n",
    "5. **‘Stop Words’ Identification**: A stop word is a commonly used word (such as “the”, “a”, “an”, “in”) that a search engine has been programmed to ignore, both when indexing entries for searching and when retrieving them as the result of a search query. We would not want these words to take up space in our database, or taking up valuable processing time. For this, we can remove them easily, by storing a list of words that you consider to stop words. NLTK(Natural Language Toolkit) in python has a list of stopwords stored in 16 different languages.\n",
    "\n",
    "6. **Case Conversion**: Converting all your data to lowercase helps in the process of preprocessing and in later stages in the NLP application, when we are doing parsing.\n",
    "\n",
    "\n",
    "These steps are implemented in the next section and will be used to build a text normalization pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acting-mandate",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup as bs \n",
    "import unicodedata\n",
    "import re\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from textblob import TextBlob\n",
    "import pycountry\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from contractions import contractions_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97427823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "# Stop words\n",
    "stopword_list = nltk.corpus.stopwords.words('english')\n",
    "stopword_list.remove('no')\n",
    "stopword_list.remove('not')\n",
    "\n",
    "# Lemmatization\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "nlp.max_length = 1500000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "civil-chick",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = pd.read_csv('../data/interim/covid_articles_preprocessed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satellite-demographic",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4830b9a",
   "metadata": {},
   "source": [
    "### Noize Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "looking-bryan",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove html tages\n",
    "def strip_html_tags(text):\n",
    "    soup = bs(text, \"html.parser\")\n",
    "    stripped_text = soup.get_text()\n",
    "    return stripped_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "literary-bangkok",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove accented text\n",
    "def remove_accented_chars(text):\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compressed-invitation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Special Characters\n",
    "def remove_special_characters(text, remove_digits=False):\n",
    "    pattern = r'[^a-zA-z0-9\\s]' if not remove_digits else r'[^a-zA-z\\s]'\n",
    "    text = re.sub(pattern, '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "falling-colleague",
   "metadata": {},
   "source": [
    "### Contraction mapping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reasonable-demographic",
   "metadata": {},
   "outputs": [],
   "source": [
    "#contractions = CONTRACTION_MAP\n",
    "cotractions = contractions_dict\n",
    "def expand_contractions(text, contraction_mapping=contractions):\n",
    "    \n",
    "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), \n",
    "                                      flags=re.IGNORECASE|re.DOTALL)\n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        expanded_contraction = contraction_mapping.get(match)\\\n",
    "                                if contraction_mapping.get(match)\\\n",
    "                                else contraction_mapping.get(match.lower())                       \n",
    "        expanded_contraction = first_char+expanded_contraction[1:]\n",
    "        return expanded_contraction\n",
    "        \n",
    "    expanded_text = contractions_pattern.sub(expand_match, text)\n",
    "    expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
    "    return expanded_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latest-threat",
   "metadata": {},
   "source": [
    "### Spelling Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "necessary-anaheim",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct misspelled words\n",
    "def spell_correction(text):\n",
    "    tb = TextBlob(text)\n",
    "    fixed_text = tb.correct()\n",
    "    return str(fixed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "functioning-southeast",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing Meaningless strings\n",
    "def remove_incomprehensible_words(text):\n",
    "    en_words = set(nltk.corpus.words.words())\n",
    "    text = \" \".join(w for w in nltk.wordpunct_tokenize(text) \\\n",
    "     if w.lower() in en_words or not w.isalpha())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impossible-indicator",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "magnetic-number",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_text(text, disable = ['ner', 'parser']):\n",
    "    text = nlp(text)\n",
    "    text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "original-editor",
   "metadata": {},
   "source": [
    "### Remove Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "geological-commons",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text, is_lower_case=False):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    if is_lower_case:\n",
    "        filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "    else:\n",
    "        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n",
    "    filtered_text = ' '.join(filtered_tokens)    \n",
    "    return filtered_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comfortable-voltage",
   "metadata": {},
   "source": [
    "### Case conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yellow-custom",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_text(text, case='lower'):\n",
    "    if case == 'lower':\n",
    "        text = text.lower()\n",
    "    elif case == 'upper':\n",
    "        text = text.upper()\n",
    "    elif case == 'title':\n",
    "        text = text.title()\n",
    "    else:\n",
    "        text\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "responsible-graph",
   "metadata": {},
   "source": [
    "### Corpus Normalization Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mighty-farmer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_corpus(corpus, html_stripping=True, contraction_expansion=True,\n",
    "                     accented_char_removal=True, text_lower_case=True, \n",
    "                     text_lemmatization=True, special_char_removal=True, \n",
    "                     stopword_removal=True, remove_digits=True,\n",
    "                     correct_spelling=False, remove_incomprehensible_word=True):\n",
    "    \n",
    "    normalized_corpus = []\n",
    "    # normalize each document in the corpus\n",
    "    for doc in corpus:\n",
    "        # strip HTML\n",
    "        if html_stripping:\n",
    "            doc = strip_html_tags(doc)\n",
    "        # remove accented characters\n",
    "        if accented_char_removal:\n",
    "            doc = remove_accented_chars(doc)\n",
    "        # expand contractions    \n",
    "        if contraction_expansion:\n",
    "            doc = expand_contractions(doc)\n",
    "        # lowercase the text    \n",
    "        if text_lower_case:\n",
    "            doc = doc.lower()\n",
    "        # remove extra newlines\n",
    "        doc = re.sub(r'[\\r|\\n|\\r\\n]+', ' ',doc)\n",
    "        # lemmatize text\n",
    "        if text_lemmatization:\n",
    "            doc = lemmatize_text(doc)\n",
    "        # remove special characters and\\or digits    \n",
    "        if special_char_removal:\n",
    "            # insert spaces between special characters to isolate them    \n",
    "            special_char_pattern = re.compile(r'([{._(-)!}])')\n",
    "            doc = special_char_pattern.sub(\" \\\\1 \", doc)\n",
    "            doc = remove_special_characters(doc, remove_digits=remove_digits)  \n",
    "        # remove extra whitespace\n",
    "        doc = re.sub(' +', ' ', doc)\n",
    "        # remove stopwords\n",
    "        if stopword_removal:\n",
    "            doc = remove_stopwords(doc, is_lower_case=text_lower_case)\n",
    "        # correct spelling\n",
    "        if correct_spelling:\n",
    "            doc = spell_correction(doc)\n",
    "        # remove incomprehensible words\n",
    "        if remove_incomprehensible_word:\n",
    "            doc = remove_incomprehensible_words(doc)\n",
    "            \n",
    "        normalized_corpus.append(doc)\n",
    "        \n",
    "    return normalized_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "signed-shaft",
   "metadata": {},
   "source": [
    "## Normalize COVID News Articles\n",
    "Now the normaliztion pipeline is defined, I normalize the covid articles and save them for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moving-semester",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df = df\n",
    "text_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "super-turkey",
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_corpus = normalize_corpus(text_df['content'].values)\n",
    "print(normal_corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "agreed-citation",
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_corpus_df = pd.concat((text_df[['title', 'topic_area']], pd.DataFrame(normal_corpus, columns=['content'])), axis=1)\n",
    "normal_corpus_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c37d3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_corpus_df.to_csv('../data/interim/covid_articles_normalized.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
