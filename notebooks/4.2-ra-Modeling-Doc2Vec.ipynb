{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51362744",
   "metadata": {},
   "source": [
    "# Doc2Vec Model\n",
    "In this notebook I am using Gensim API to learn paragraph and document embeddings via the distributed memory and distributed bag of words models from Quoc Le and Tomas Mikolov: “Distributed Representations of Sentences and Documents”.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6322f00d",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "Word2vec, created by a team of researchers at Google led by Tomáš Mikolov, implements a word embedding model that enables us to create these kinds of distributed representations. The word2vec algorithm trains word representations based on either a continuous bag-of-words (CBOW) or skip-gram model, such that words are embedded in space along with similar words based on their context. For example, Gensim’s implementation uses a feedforward network.\n",
    "\n",
    "\n",
    "The doc2vec algorithm is an extension of word2vec. It proposes a paragraph vector—an unsupervised algorithm that learns fixed-length feature representations from variable length documents. This representation attempts to inherit the semantic properties of words such that “red” and “colorful” are more similar to each other than they are to “river” or “governance.” Moreover, the paragraph vector takes into consideration the ordering of words within a narrow context, similar to an n-gram model. The combined result is much more effective than a bag-of-words or bag-of-n-grams model because it generalizes better and has a lower dimensionality but still is of a fixed length so it can be used in common machine learning algorithms.\n",
    "\n",
    "##### The Gensim way\n",
    "Neither NLTK nor Scikit-Learn provide implementations of these kinds of word embeddings. Gensim’s implementation allows users to train both word2vec and doc2vec models on custom corpora and also conveniently comes with a model that is pretrained on the Google news corpus.\n",
    "\n",
    "To train the model first, I load the corpus into memory and create a list of TaggedDocument objects, which extend the LabeledSentence, and in turn the distributed representation of word2vec. TaggedDocument objects consist of words and tags. I can instantiate the tagged document with the list of tokens along with the article index, that uniquely identifies the instance.\n",
    "\n",
    "Once I have a list of tagged documents, the code instantiate the Doc2Vec model and specify the size of the vector as well as the minimum count, which ignores all tokens that have a frequency less than that number. Once instantiated, an unsupervised neural network is trained to learn the vector representations, which can then be accessed via the docvecs property.\n",
    "\n",
    "The model itself can be saved to disk and retrained in an active fashion, making it extremely flexible for a variety of use cases. However, on larger corpora, training can be slow and memory intensive, and it might not be as good as a TF–IDF model with Principal Component Analysis (PCA) or Singular Value Decomposition (SVD) applied to reduce the feature space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ca70e4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import libraries\n",
    "\n",
    "## General\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from pprint import pprint\n",
    "import time\n",
    "\n",
    "## Gensim\n",
    "from gensim.models.doc2vec import TaggedDocument, Doc2Vec\n",
    "from gensim.utils import tokenize, simple_preprocess\n",
    "\n",
    "\n",
    "## Scikit-Learn \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "from skopt import BayesSearchCV\n",
    "from skopt import dump, load"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368113e6",
   "metadata": {},
   "source": [
    "## 2. Import data\n",
    "First I load the pre-processed text into the notebook and use the categories I generated in the previous notebook to merge less frequent categories with the most appropriate ones. The aim of this step is to reduce the number of categories to 5. The reason for this apporach is the imbalance among the categories in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1d61343b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "business    245652\n",
       "general      86372\n",
       "finance      22386\n",
       "tech          8915\n",
       "science       5595\n",
       "Name: tags, dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the data\n",
    "df_normal_text = pd.read_csv('../data/interim/covid_articles_normalized.csv')\n",
    "\n",
    "## Merge Tags\n",
    "\n",
    "tag_map = {'consumer':'general',\n",
    "           'healthcare':'science',\n",
    "           'automotive':'business',\n",
    "           'environment':'science',\n",
    "           'construction':'business',\n",
    "           'ai':'tech'}\n",
    "\n",
    "df_normal_text['tags'] = [(lambda tags: tag_map[tags] if tags in tag_map.keys() else tags)(tags)\n",
    "                          for tags in df_normal_text['topic_area']]\n",
    "df_normal_text.tags.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a252e27d",
   "metadata": {},
   "source": [
    "## 3. Gensim Doc2Vec model\n",
    "This step involves \n",
    "* **Splitting the data in train and test groups**: I use the Scikit-Learn train-test split model with a ratio of 30% test data. \n",
    "* **Tokenizing and tagging the articles**: Gensim has a tokenizer to which create word tokens. I used article index as the tag for each entry. Tags are unique IDs for each article used to look-up the learned vectors after training. \n",
    "* **Initializing the model**: Training a Doc2Vec model is a memory intensive process. I had to adopt measures to fit the data in memory. A tool for managing the size of the model is the vector_size which indicates the dimensionality of the feature vectors. I set it to 100 for the base case. The default number of iterations (epochs) over the corpus is 10 for Doc2Vec. Typical iteration counts in the published Paragraph Vector paper results, using 10s-of-thousands to millions of docs, are 10-20. More iterations take more time and eventually reach a point of diminishing returns. I used 15 epochs for the base case and will try different values with bayes search.\n",
    "* **Building the vocabulary dictionary**: The vocabulary is a list of all of the unique words extracted from the training corpus.\n",
    "* **training the doc2wev NN**: Use the ```model.train``` to fit the create embedings for each article.\n",
    "* **Infer Vectors**: I use the trained model to infer a vector for any piece of text by passing a list of words to the ```model.infer_vector``` function. This vector can then be compared with other vectors via cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6e4f76da",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Split the data in train and test\n",
    "train, test = train_test_split(df_normal_text[['content', 'tags']], test_size=0.3, random_state=21)\n",
    "\n",
    "## Tokenizing and tagging the articles\n",
    "train_tagged=[]\n",
    "for index, row in train.iterrows():\n",
    "    train_tagged.append(TaggedDocument(words=list(tokenize(row.content)), tags=str(index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b9baa2c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TaggedDocument(words=['send', 'aviation', 'industry', 'liquidity', 'strap', 'across', 'world', 'send', 'administration', 'part', 'government', 'ownership', 'follow', 'global', 'massive', 'reduction', 'air', 'travel', 'international', 'air', 'transport', 'association', 'predict', 'sector', 'head', 'us', 'billion', 'billion', 'net', 'loss', 'direct', 'result', 'covid', 'many', 'struggle', 'navigate', 'uncharted', 'territory', 'one', 'survive', 'crisis', 'strategically', 'creative', 'must', 'find', 'way', 'public', 'health', 'maintain', 'profitability', 'one', 'way', 'embrace', 'ultra', 'long', 'haul', 'flight', 'research', 'aviation', 'consultant', 'scholar', 'university', 'investigate', 'operator', 'ultra', 'long', 'haul', 'flight', 'capacity', 'like', 'well', 'place', 'new', 'normal', 'emerge', 'people', 'priority', 'shift', 'toward', 'concern', 'health', 'ultra', 'long', 'haul', 'flight', 'significant', 'benefit', 'current', 'common', 'model', 'stop', 'change', 'plane', 'big', 'hub', 'midway', 'journey', 'technology', 'fly', 'long', 'distance', 'exist', 'previously', 'not', 'economical', 'make', 'sense', 'transport', 'people', 'major', 'hub', 'big', 'plane', 'switch', 'onto', 'small', 'one', 'final', 'part', 'journey', 'big', 'middle', 'eastern', 'emirate', 'build', 'hub', 'speak', 'model', 'new', 'aircraft', 'game', 'changer', 'economic', 'long', 'distance', 'travel', 'twin', 'engine', 'fuel', 'efficient', 'long', 'range', 'aircraft', 'moderate', 'cabin', 'density', 'large', 'share', 'premium', 'seat', 'new', 'aircraft', 'well', 'environment', 'people', 'willing', 'pay', 'travel', 'particularly', 'avoid', 'transit', 'busy', 'hub', 'willingness', 'pay', 'even', 'great', 'among', 'business', 'traveller', 'visit', 'friend', 'relative', 'time', 'travel', 'not', 'advise', 'first', 'fly', 'nonstop', 'new', 'york', 'record', 'hour', 'flight', 'use', 'well', 'power', 'fly', 'far', 'plane', 'also', 'make', 'economical', 'fly', 'long', 'haul', 'flight', 'less', 'popular', 'destination', 'build', 'pandemic', 'industry', 'face', 'crippling', 'overcapacity', 'issue', 'huge', 'level', 'competition', 'result', 'shrink', 'pool', 'profit', 'many', 'way', 'industry', 'compete', 'death', 'fare', 'plummet', 'cheap', 'level', 'financial', 'environmental', 'point', 'view', 'turn', 'quality', 'service', 'decline', 'effective', 'portion', 'market', 'increasingly', 'overbearing', 'approach', 'not', 'attractive', 'lot', 'customer', 'many', 'long', 'access', 'well', 'product', 'service', 'yet', 'several', 'fall', 'victim', 'simply', 'follow', 'herd', 'cut', 'cost', 'wherever', 'could', 'pursuit', 'ultra', 'long', 'haul', 'model', 'pursue', 'different', 'strategy', 'instead', 'compete', 'saturated', 'space', 'try', 'eke', 'could', 'exist', 'level', 'demand', 'create', 'new', 'uncontested', 'market', 'ultra', 'long', 'haul', 'service', 'also', 'eliminate', 'feature', 'board', 'duty', 'free', 'sale', 'thereby', 'reduce', 'weight', 'cost', 'meanwhile', 'marketing', 'strategy', 'include', 'menu', 'promote', 'hydration', 'rest', 'well', 'state', 'art', 'transit', 'lounge', 'main', 'entry', 'point', 'dedicate', 'ultra', 'long', 'haul', 'service', 'emphasis', 'health', 'wellness', 'could', 'prove', 'popular', 'month', 'come', 'ultimately', 'enable', 'capture', 'whole', 'new', 'demographic', 'traveller', 'may', 'previously', 'opt', 'take', 'cheap', 'option', 'include', 'stopover', 'middle', 'east', 'also', 'appeal', 'segment', 'premium', 'customer', 'typically', 'fly', 'degree', 'frequency', 'traveller', 'open', 'pay', 'premium', 'fare', 'around', 'go', 'rate', 'high', 'quality', 'efficient', 'service', 'recovery', 'not', 'easy', 'pivot', 'easily', 'new', 'normal', 'well', 'place', 'prosper', 'may', 'see', 'launch', 'ultra', 'long', 'haul', 'route', 'united', 'drive', 'factor', 'include', 'change', 'customer', 'behaviour', 'geographical', 'location', 'city', 'deployment', 'efficient', 'aircraft', 'ongoing', 'travel', 'restriction', 'establishment', 'travel', 'bubble', 'certain', 'country', 'control', 'increase', 'demand', 'direct', 'service', 'although', 'golden', 'opportunity', 'ultra', 'long', 'haul', 'sector', 'remain', 'niche', 'market', 'realistic', 'opportunity', 'post', 'era', 'article', 'republish', 'conversation', 'creative', 'common', 'license', 'read', 'original', 'article', 'manage', 'director', 'aviation', 'consult', 'firm', 'aviation', 'advisory'], tags='226001')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Let's look at a tagged article\n",
    "train_tagged[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "47319214",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialize the model with vector size and number of epochs\n",
    "model = Doc2Vec(vector_size=100, epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "86aa5312",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Building the vocabulary dictionary\n",
    "model.build_vocab(train_tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e1e86384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 'vaccine' appeared 124781 times in the training corpus.\n"
     ]
    }
   ],
   "source": [
    "## Let's check how mnay times a word appeared in the corpus.\n",
    "print(f\"Word 'vaccine' appeared {model.wv.get_vecattr('vaccine', 'count')} times in the training corpus.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0e5974b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training the model\n",
    "model.train(pd.Series(train_tagged).values, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ecca5b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "## A function to infer the documents\n",
    "def vectorize(model, corpus):\n",
    "    regressors, tags = zip(*[(model.infer_vector(doc[0].split(), steps=20), doc[1]) for doc in corpus.values])\n",
    "    return regressors, tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "93db1133",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = vectorize(model, train)\n",
    "X_test, y_test = vectorize(model, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "51fd87bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save model\n",
    "#model.save('../models/gensim-model-doc2vec-vs300')\n",
    "\n",
    "## Load model\n",
    "#new_model = gensim.models.Word2Vec.load('../models/gensim-model-doc2vec-')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2708ba83",
   "metadata": {},
   "source": [
    "## 4. Classification Algorithm - Logistic Regression\n",
    "In the previous notebook (classification with Tfidf), I concluded that showed that logistic regression is the most appropriate model for classsification of articles in this dataset. So, I will use logistic regression to classifiy the articles using the embeding generated with the doc2vec model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9b80bc2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(solver='liblinear')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_lr = LogisticRegression(solver='liblinear')\n",
    "clf_lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "065a5267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    business       0.76      0.92      0.83     73674\n",
      "     finance       0.52      0.01      0.01      6763\n",
      "     general       0.69      0.55      0.62     25951\n",
      "     science       0.51      0.09      0.15      1614\n",
      "        tech       0.50      0.07      0.12      2674\n",
      "\n",
      "    accuracy                           0.75    110676\n",
      "   macro avg       0.60      0.33      0.35    110676\n",
      "weighted avg       0.72      0.75      0.71    110676\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf_lr.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7621eb0e",
   "metadata": {},
   "source": [
    "The results indicate that the class imbalance causes issues for classification of less populated categories. I've seen similar behaviour with Tfidf. In the next step I use bayes search to explore the hyperparamter space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2015d510",
   "metadata": {},
   "source": [
    "## 5. Hyper-parameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d456fbf4",
   "metadata": {},
   "source": [
    "### 5.1. Define a Doc2Vec Class To Integrate with Scikit-Learn API\n",
    "In order to use the ```bayessearchcv``` I have to create a wrapper around the Gensim's Doc2Vec model that uses a similar structure as native Scikit-Learn classes. I passed the doc2vec parameters to ```__init__``` and then defined the tagging, tokenization, building vocabulary, and training in the fit method. The transform method retruns the infer_vector for train and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "83b010ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Doc2VecModel(BaseEstimator):\n",
    "\n",
    "    def __init__(self, dm=1, vector_size=100, window=1, epochs=10, max_vocab_size=12e7, min_count=1):\n",
    "        #print('>>>>>>>>init() called.\\n')\n",
    "        self.d2v_model = None\n",
    "        self.vector_size = vector_size\n",
    "        self.window = window\n",
    "        self.dm = dm\n",
    "        self.epochs = epochs\n",
    "        self.max_vocab_size = max_vocab_size\n",
    "        self.min_count = min_count\n",
    "\n",
    "    def fit(self, corpus, y=None):\n",
    "        #print('>>>>>>>>fit() called.\\n')\n",
    "        ## Initialize model\n",
    "        self.d2v_model = Doc2Vec(vector_size=self.vector_size, window=self.window, dm=self.dm, epochs=self.epochs,\n",
    "                                 max_vocab_size=self.max_vocab_size, min_count=self.min_count,\n",
    "                                 alpha=0.025, min_alpha=0.001, seed=21)\n",
    "        ## Tag docs\n",
    "        docs_tagged=[]\n",
    "        for index, row in corpus.iteritems():\n",
    "            docs_tagged.append(TaggedDocument(words=list(tokenize(row)), tags=str(index)))\n",
    "        ## Build vocabulary\n",
    "        self.d2v_model.build_vocab(docs_tagged)\n",
    "        ## Train model\n",
    "        self.d2v_model.train(pd.Series(docs_tagged).values, total_examples=self.d2v_model.corpus_count, epochs=self.d2v_model.epochs)\n",
    "        return self\n",
    "\n",
    "    def transform(self, corpus):\n",
    "        #print('>>>>>>>>transform() called.\\n')\n",
    "        sents = corpus.values\n",
    "        regressors = [(self.d2v_model.infer_vector(doc[0].split(), steps=20)) for doc in sents]\n",
    "        regressors = pd.DataFrame(regressors, index=corpus.index)\n",
    "        return regressors\n",
    "\n",
    "\n",
    "    def fit_transform(self, corpus, y=None):\n",
    "        self.fit(corpus)\n",
    "        return self.transform(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d562e0",
   "metadata": {},
   "source": [
    "### 5.2. Search Configuration\n",
    "The next step is to define the pipeline and parameter grid for bayessearch. \n",
    "* Doc2Vec hyperparameters\n",
    "    * window – The maximum distance between the current and predicted word within a sentence.\n",
    "    * dm -  Defines the training algorithm. If dm=1, ‘distributed memory’ (PV-DM) is used. Otherwise, distributed bag of words (PV-DBOW) is employed. \n",
    "    * vector_size – Dimensionality of the feature vectors.\n",
    "    * min_count – Ignores all words with total frequency lower than this.\n",
    "    * epochs – Number of iterations (epochs) over the corpus. Defaults to 10 for Doc2Vec.\n",
    "    \n",
    "The bayessearchcv with doc2vec model is a memory intensive process. To handle this task I had to limit the number of parallel processors. Since the doc2vec takes a longtime to create the embeddings and I had limited parallel workers, number of bayessearch iterations and crossfolds are reduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "24507477",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model Specifications\n",
    "# Create pipeline\n",
    "model_pipe = Pipeline([('vect', Doc2VecModel()),\n",
    "                      ('clf', LogisticRegression(solver='liblinear'))])\n",
    "\n",
    "# Parameter grid\n",
    "param_grid = {'vect__window': list(range(5)),\n",
    "              'vect__dm': [0,1],\n",
    "              'vect__vector_size': list(range(100,400)),\n",
    "              'vect__min_count': list(range(100)),\n",
    "              'vect__epochs': [10,15,20,25,30],\n",
    "              'clf__dual': (True,False),\n",
    "              'clf__max_iter': [100,110,120,130,140],\n",
    "              'clf__C': (1e-5, 1e2, \"log-uniform\"),\n",
    "}\n",
    "\n",
    "# Number of iterations: Number of parameter settings that are sampled.\n",
    "n_iter = 15\n",
    "\n",
    "# Split the data to train and test\n",
    "train, test = train_test_split(df_normal_text[['content', 'tags']], test_size=0.3, random_state=21)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fba3ad",
   "metadata": {},
   "source": [
    "### 5.3. BayesSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "38b6c563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing grid search...\n",
      "pipeline: ['vect', 'clf']\n",
      "parameters:\n",
      "vect__window: [0, 1, 2, 3, 4]\n",
      "vect__dm: [0, 1]\n",
      "vect__vector_size: [100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399]\n",
      "vect__min_count: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]\n",
      "vect__epochs: [10, 15, 20, 25, 30]\n",
      "clf__dual: (True, False)\n",
      "clf__max_iter: [100, 110, 120, 130, 140]\n",
      "clf__C: (1e-05, 100.0, 'log-uniform')\n",
      "Fitting 4 folds for each of 1 candidates, totalling 4 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done   4 out of   4 | elapsed: 65.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 1 candidates, totalling 4 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done   4 out of   4 | elapsed: 342.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 1 candidates, totalling 4 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done   4 out of   4 | elapsed: 163.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 1 candidates, totalling 4 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done   4 out of   4 | elapsed: 108.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 1 candidates, totalling 4 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done   4 out of   4 | elapsed: 311.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 1 candidates, totalling 4 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done   4 out of   4 | elapsed: 351.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 1 candidates, totalling 4 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done   4 out of   4 | elapsed: 145.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 1 candidates, totalling 4 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done   4 out of   4 | elapsed: 154.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 1 candidates, totalling 4 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done   4 out of   4 | elapsed: 144.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 1 candidates, totalling 4 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done   4 out of   4 | elapsed: 153.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 1 candidates, totalling 4 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done   4 out of   4 | elapsed: 266.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 1 candidates, totalling 4 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done   4 out of   4 | elapsed: 173.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 1 candidates, totalling 4 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done   4 out of   4 | elapsed: 183.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 1 candidates, totalling 4 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done   4 out of   4 | elapsed: 101.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 1 candidates, totalling 4 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done   4 out of   4 | elapsed: 136.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.691\n",
      "Best parameters set:\n",
      "\tclf__C: 0.005206429077063814\n",
      "\tclf__dual: True\n",
      "\tclf__max_iter: 120\n",
      "\tvect__dm: 0\n",
      "\tvect__epochs: 25\n",
      "\tvect__min_count: 93\n",
      "\tvect__vector_size: 219\n",
      "\tvect__window: 1\n"
     ]
    }
   ],
   "source": [
    "## Initialize the search\n",
    "bayes_search_cv = BayesSearchCV(estimator=model_pipe, search_spaces=param_grid,\n",
    "                                n_iter=n_iter, n_jobs=2, verbose=1, random_state=21, cv=4)\n",
    "\n",
    "## Print search configurations\n",
    "print(\"Performing grid search...\")\n",
    "print(\"pipeline:\", [name for name, _ in model_pipe.steps])\n",
    "print(\"parameters:\")\n",
    "for key, value in param_grid.items():\n",
    "    print('{}: {}'.format(key, value))\n",
    "\n",
    "## Run the search\n",
    "bayes_search_cv.fit(train.content, train.tags.values)\n",
    "\n",
    "## Print best parameters and results\n",
    "print(\"Best score: %0.3f\" % bayes_search_cv.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = bayes_search_cv.best_estimator_.get_params()\n",
    "\n",
    "for param_name in sorted(param_grid.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ba51d7e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rasaee\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    business       0.70      0.96      0.81     73674\n",
      "     finance       0.00      0.00      0.00      6763\n",
      "     general       0.62      0.22      0.33     25951\n",
      "     science       0.00      0.00      0.00      1614\n",
      "        tech       0.00      0.00      0.00      2674\n",
      "\n",
      "    accuracy                           0.69    110676\n",
      "   macro avg       0.26      0.24      0.23    110676\n",
      "weighted avg       0.61      0.69      0.62    110676\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred =bayes_search_cv.predict(test.content)\n",
    "y_test = test.tags.values\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "98aa594b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'list'>,\n",
      "            {'mean_fit_time': [1908.7735111117363,\n",
      "                               10165.226145923138,\n",
      "                               4814.506383061409,\n",
      "                               3139.0467371940613,\n",
      "                               9222.512724280357,\n",
      "                               10486.558747112751,\n",
      "                               4255.642635285854,\n",
      "                               4540.6362399458885,\n",
      "                               4223.315940797329,\n",
      "                               4544.508498668671,\n",
      "                               7915.9643377661705,\n",
      "                               5124.964894413948,\n",
      "                               5392.34798771143,\n",
      "                               2971.5023089647293,\n",
      "                               3988.6643110513687],\n",
      "             'mean_score_time': [37.514176428318024,\n",
      "                                 39.1325164437294,\n",
      "                                 81.95469522476196,\n",
      "                                 60.08116686344147,\n",
      "                                 35.61356657743454,\n",
      "                                 40.6072279214859,\n",
      "                                 68.19351202249527,\n",
      "                                 71.75077694654465,\n",
      "                                 70.61761665344238,\n",
      "                                 25.026507139205933,\n",
      "                                 37.59915256500244,\n",
      "                                 31.90716242790222,\n",
      "                                 81.46306174993515,\n",
      "                                 55.05076593160629,\n",
      "                                 69.01364797353745],\n",
      "             'mean_test_score': [0.6910789795697093,\n",
      "                                 0.6659515806756401,\n",
      "                                 0.6910867241833305,\n",
      "                                 0.6659515806756401,\n",
      "                                 0.6910905964901412,\n",
      "                                 0.6910867241833305,\n",
      "                                 0.6910867241833305,\n",
      "                                 0.6910867241833305,\n",
      "                                 0.6910828518765199,\n",
      "                                 0.6910867241833305,\n",
      "                                 0.6910867241833305,\n",
      "                                 0.691071234956088,\n",
      "                                 0.6659515806756401,\n",
      "                                 0.6659515806756401,\n",
      "                                 0.6910905964901412],\n",
      "             'param_clf__C': [0.011789467647137593,\n",
      "                              1.3078503580653154e-05,\n",
      "                              0.045894302978616026,\n",
      "                              0.0001390206052124874,\n",
      "                              0.005206429077063814,\n",
      "                              33.63295708080422,\n",
      "                              3.4583738557971206,\n",
      "                              0.5151488514621588,\n",
      "                              1.3206264581894684,\n",
      "                              62.3905448341682,\n",
      "                              0.22403599773596372,\n",
      "                              100.0,\n",
      "                              1e-05,\n",
      "                              0.0017562067873471167,\n",
      "                              6.319477102637779],\n",
      "             'param_clf__dual': [False,\n",
      "                                 False,\n",
      "                                 False,\n",
      "                                 True,\n",
      "                                 True,\n",
      "                                 False,\n",
      "                                 False,\n",
      "                                 False,\n",
      "                                 True,\n",
      "                                 False,\n",
      "                                 True,\n",
      "                                 True,\n",
      "                                 False,\n",
      "                                 False,\n",
      "                                 True],\n",
      "             'param_clf__max_iter': [110,\n",
      "                                     130,\n",
      "                                     110,\n",
      "                                     110,\n",
      "                                     120,\n",
      "                                     110,\n",
      "                                     110,\n",
      "                                     110,\n",
      "                                     110,\n",
      "                                     100,\n",
      "                                     140,\n",
      "                                     140,\n",
      "                                     110,\n",
      "                                     140,\n",
      "                                     100],\n",
      "             'param_vect__dm': [1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1],\n",
      "             'param_vect__epochs': [10,\n",
      "                                    30,\n",
      "                                    30,\n",
      "                                    20,\n",
      "                                    25,\n",
      "                                    30,\n",
      "                                    25,\n",
      "                                    25,\n",
      "                                    25,\n",
      "                                    15,\n",
      "                                    30,\n",
      "                                    10,\n",
      "                                    30,\n",
      "                                    15,\n",
      "                                    25],\n",
      "             'param_vect__min_count': [83,\n",
      "                                       31,\n",
      "                                       63,\n",
      "                                       20,\n",
      "                                       93,\n",
      "                                       61,\n",
      "                                       16,\n",
      "                                       56,\n",
      "                                       55,\n",
      "                                       21,\n",
      "                                       99,\n",
      "                                       99,\n",
      "                                       99,\n",
      "                                       6,\n",
      "                                       99],\n",
      "             'param_vect__vector_size': [319,\n",
      "                                         178,\n",
      "                                         266,\n",
      "                                         257,\n",
      "                                         219,\n",
      "                                         202,\n",
      "                                         291,\n",
      "                                         351,\n",
      "                                         376,\n",
      "                                         124,\n",
      "                                         100,\n",
      "                                         399,\n",
      "                                         391,\n",
      "                                         369,\n",
      "                                         308],\n",
      "             'param_vect__window': [4,\n",
      "                                    2,\n",
      "                                    3,\n",
      "                                    1,\n",
      "                                    1,\n",
      "                                    1,\n",
      "                                    2,\n",
      "                                    2,\n",
      "                                    1,\n",
      "                                    1,\n",
      "                                    0,\n",
      "                                    0,\n",
      "                                    1,\n",
      "                                    3,\n",
      "                                    2],\n",
      "             'params': [OrderedDict([('clf__C', 0.011789467647137593),\n",
      "                                     ('clf__dual', False),\n",
      "                                     ('clf__max_iter', 110),\n",
      "                                     ('vect__dm', 1),\n",
      "                                     ('vect__epochs', 10),\n",
      "                                     ('vect__min_count', 83),\n",
      "                                     ('vect__vector_size', 319),\n",
      "                                     ('vect__window', 4)]),\n",
      "                        OrderedDict([('clf__C', 1.3078503580653154e-05),\n",
      "                                     ('clf__dual', False),\n",
      "                                     ('clf__max_iter', 130),\n",
      "                                     ('vect__dm', 0),\n",
      "                                     ('vect__epochs', 30),\n",
      "                                     ('vect__min_count', 31),\n",
      "                                     ('vect__vector_size', 178),\n",
      "                                     ('vect__window', 2)]),\n",
      "                        OrderedDict([('clf__C', 0.045894302978616026),\n",
      "                                     ('clf__dual', False),\n",
      "                                     ('clf__max_iter', 110),\n",
      "                                     ('vect__dm', 1),\n",
      "                                     ('vect__epochs', 30),\n",
      "                                     ('vect__min_count', 63),\n",
      "                                     ('vect__vector_size', 266),\n",
      "                                     ('vect__window', 3)]),\n",
      "                        OrderedDict([('clf__C', 0.0001390206052124874),\n",
      "                                     ('clf__dual', True),\n",
      "                                     ('clf__max_iter', 110),\n",
      "                                     ('vect__dm', 1),\n",
      "                                     ('vect__epochs', 20),\n",
      "                                     ('vect__min_count', 20),\n",
      "                                     ('vect__vector_size', 257),\n",
      "                                     ('vect__window', 1)]),\n",
      "                        OrderedDict([('clf__C', 0.005206429077063814),\n",
      "                                     ('clf__dual', True),\n",
      "                                     ('clf__max_iter', 120),\n",
      "                                     ('vect__dm', 0),\n",
      "                                     ('vect__epochs', 25),\n",
      "                                     ('vect__min_count', 93),\n",
      "                                     ('vect__vector_size', 219),\n",
      "                                     ('vect__window', 1)]),\n",
      "                        OrderedDict([('clf__C', 33.63295708080422),\n",
      "                                     ('clf__dual', False),\n",
      "                                     ('clf__max_iter', 110),\n",
      "                                     ('vect__dm', 0),\n",
      "                                     ('vect__epochs', 30),\n",
      "                                     ('vect__min_count', 61),\n",
      "                                     ('vect__vector_size', 202),\n",
      "                                     ('vect__window', 1)]),\n",
      "                        OrderedDict([('clf__C', 3.4583738557971206),\n",
      "                                     ('clf__dual', False),\n",
      "                                     ('clf__max_iter', 110),\n",
      "                                     ('vect__dm', 1),\n",
      "                                     ('vect__epochs', 25),\n",
      "                                     ('vect__min_count', 16),\n",
      "                                     ('vect__vector_size', 291),\n",
      "                                     ('vect__window', 2)]),\n",
      "                        OrderedDict([('clf__C', 0.5151488514621588),\n",
      "                                     ('clf__dual', False),\n",
      "                                     ('clf__max_iter', 110),\n",
      "                                     ('vect__dm', 1),\n",
      "                                     ('vect__epochs', 25),\n",
      "                                     ('vect__min_count', 56),\n",
      "                                     ('vect__vector_size', 351),\n",
      "                                     ('vect__window', 2)]),\n",
      "                        OrderedDict([('clf__C', 1.3206264581894684),\n",
      "                                     ('clf__dual', True),\n",
      "                                     ('clf__max_iter', 110),\n",
      "                                     ('vect__dm', 1),\n",
      "                                     ('vect__epochs', 25),\n",
      "                                     ('vect__min_count', 55),\n",
      "                                     ('vect__vector_size', 376),\n",
      "                                     ('vect__window', 1)]),\n",
      "                        OrderedDict([('clf__C', 62.3905448341682),\n",
      "                                     ('clf__dual', False),\n",
      "                                     ('clf__max_iter', 100),\n",
      "                                     ('vect__dm', 0),\n",
      "                                     ('vect__epochs', 15),\n",
      "                                     ('vect__min_count', 21),\n",
      "                                     ('vect__vector_size', 124),\n",
      "                                     ('vect__window', 1)]),\n",
      "                        OrderedDict([('clf__C', 0.22403599773596372),\n",
      "                                     ('clf__dual', True),\n",
      "                                     ('clf__max_iter', 140),\n",
      "                                     ('vect__dm', 0),\n",
      "                                     ('vect__epochs', 30),\n",
      "                                     ('vect__min_count', 99),\n",
      "                                     ('vect__vector_size', 100),\n",
      "                                     ('vect__window', 0)]),\n",
      "                        OrderedDict([('clf__C', 100.0),\n",
      "                                     ('clf__dual', True),\n",
      "                                     ('clf__max_iter', 140),\n",
      "                                     ('vect__dm', 0),\n",
      "                                     ('vect__epochs', 10),\n",
      "                                     ('vect__min_count', 99),\n",
      "                                     ('vect__vector_size', 399),\n",
      "                                     ('vect__window', 0)]),\n",
      "                        OrderedDict([('clf__C', 1e-05),\n",
      "                                     ('clf__dual', False),\n",
      "                                     ('clf__max_iter', 110),\n",
      "                                     ('vect__dm', 1),\n",
      "                                     ('vect__epochs', 30),\n",
      "                                     ('vect__min_count', 99),\n",
      "                                     ('vect__vector_size', 391),\n",
      "                                     ('vect__window', 1)]),\n",
      "                        OrderedDict([('clf__C', 0.0017562067873471167),\n",
      "                                     ('clf__dual', False),\n",
      "                                     ('clf__max_iter', 140),\n",
      "                                     ('vect__dm', 1),\n",
      "                                     ('vect__epochs', 15),\n",
      "                                     ('vect__min_count', 6),\n",
      "                                     ('vect__vector_size', 369),\n",
      "                                     ('vect__window', 3)]),\n",
      "                        OrderedDict([('clf__C', 6.319477102637779),\n",
      "                                     ('clf__dual', True),\n",
      "                                     ('clf__max_iter', 100),\n",
      "                                     ('vect__dm', 1),\n",
      "                                     ('vect__epochs', 25),\n",
      "                                     ('vect__min_count', 99),\n",
      "                                     ('vect__vector_size', 308),\n",
      "                                     ('vect__window', 2)])],\n",
      "             'rank_test_score': [10,\n",
      "                                 12,\n",
      "                                 3,\n",
      "                                 12,\n",
      "                                 1,\n",
      "                                 3,\n",
      "                                 3,\n",
      "                                 3,\n",
      "                                 9,\n",
      "                                 3,\n",
      "                                 3,\n",
      "                                 11,\n",
      "                                 12,\n",
      "                                 12,\n",
      "                                 1],\n",
      "             'split0_test_score': [0.6911757872399746,\n",
      "                                   0.6659593252892613,\n",
      "                                   0.6911757872399746,\n",
      "                                   0.6659593252892613,\n",
      "                                   0.691191276467217,\n",
      "                                   0.6911757872399746,\n",
      "                                   0.6911602980127322,\n",
      "                                   0.6911757872399746,\n",
      "                                   0.6911602980127322,\n",
      "                                   0.6911757872399746,\n",
      "                                   0.6911757872399746,\n",
      "                                   0.6911757872399746,\n",
      "                                   0.6659593252892613,\n",
      "                                   0.6659593252892613,\n",
      "                                   0.6911757872399746],\n",
      "             'split1_test_score': [0.6888833816080916,\n",
      "                                   0.6659593252892613,\n",
      "                                   0.6888988708353341,\n",
      "                                   0.6659593252892613,\n",
      "                                   0.6888988708353341,\n",
      "                                   0.6888988708353341,\n",
      "                                   0.6888988708353341,\n",
      "                                   0.6888988708353341,\n",
      "                                   0.6888988708353341,\n",
      "                                   0.6888988708353341,\n",
      "                                   0.6888988708353341,\n",
      "                                   0.6888678923808491,\n",
      "                                   0.6659593252892613,\n",
      "                                   0.6659593252892613,\n",
      "                                   0.6889143600625764],\n",
      "             'split2_test_score': [0.6929105806911293,\n",
      "                                   0.6659438360620189,\n",
      "                                   0.6929260699183718,\n",
      "                                   0.6659438360620189,\n",
      "                                   0.6929415591456142,\n",
      "                                   0.6929260699183718,\n",
      "                                   0.6929260699183718,\n",
      "                                   0.6929260699183718,\n",
      "                                   0.6929260699183718,\n",
      "                                   0.6929260699183718,\n",
      "                                   0.6929260699183718,\n",
      "                                   0.6929105806911293,\n",
      "                                   0.6659438360620189,\n",
      "                                   0.6659438360620189,\n",
      "                                   0.6929260699183718],\n",
      "             'split3_test_score': [0.6913461687396416,\n",
      "                                   0.6659438360620189,\n",
      "                                   0.6913461687396416,\n",
      "                                   0.6659438360620189,\n",
      "                                   0.6913306795123991,\n",
      "                                   0.6913461687396416,\n",
      "                                   0.691361657966884,\n",
      "                                   0.6913461687396416,\n",
      "                                   0.6913461687396416,\n",
      "                                   0.6913461687396416,\n",
      "                                   0.6913461687396416,\n",
      "                                   0.6913306795123991,\n",
      "                                   0.6659438360620189,\n",
      "                                   0.6659438360620189,\n",
      "                                   0.6913461687396416],\n",
      "             'std_fit_time': [13.048419297309907,\n",
      "                              54.25481453600043,\n",
      "                              23.3514686096406,\n",
      "                              37.22269238558962,\n",
      "                              90.97728416863139,\n",
      "                              28.55662124706657,\n",
      "                              96.90979686702964,\n",
      "                              25.122405222887707,\n",
      "                              107.59075724870112,\n",
      "                              65.73554748817801,\n",
      "                              35.02454275180917,\n",
      "                              39.9117475041059,\n",
      "                              42.633105825540454,\n",
      "                              76.5184770779027,\n",
      "                              19.010610797210855],\n",
      "             'std_score_time': [1.0943729167296044,\n",
      "                                1.013465901791635,\n",
      "                                2.8122044331122793,\n",
      "                                4.510865428159233,\n",
      "                                1.756389746941392,\n",
      "                                0.9945016868714351,\n",
      "                                2.141011479558563,\n",
      "                                3.1319978256671455,\n",
      "                                2.8168899018960145,\n",
      "                                1.4151574648991057,\n",
      "                                1.286856886966205,\n",
      "                                2.196077015772554,\n",
      "                                3.1062220546884327,\n",
      "                                3.594905030764553,\n",
      "                                1.8127185728546678],\n",
      "             'std_test_score': [0.0014366780132129655,\n",
      "                                7.744613621185614e-06,\n",
      "                                0.0014357174784068807,\n",
      "                                7.744613621185614e-06,\n",
      "                                0.0014402691079496012,\n",
      "                                0.0014357174784068807,\n",
      "                                0.0014362187071495797,\n",
      "                                0.0014357174784068807,\n",
      "                                0.0014354929129471463,\n",
      "                                0.0014357174784068807,\n",
      "                                0.0014357174784068807,\n",
      "                                0.00144188712237994,\n",
      "                                7.744613621185614e-06,\n",
      "                                7.744613621185614e-06,\n",
      "                                0.0014298201226878395]})\n"
     ]
    }
   ],
   "source": [
    "pprint(bayes_search_cv.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8eb1c1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save model\n",
    "# dump(bayes_search_cv, '../models/bayes_d2v.pkl')\n",
    "\n",
    "## Load model\n",
    "# new_model = load('../models/bayes_d2v.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57faf940",
   "metadata": {},
   "source": [
    "Running a bayessearchcv is very time intensive step. The results of bayessearch do not improve the classification results. So, I will not explore a more focused hyperparameter space and will move to another model to investigate performance of DL for the given dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df743be1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
