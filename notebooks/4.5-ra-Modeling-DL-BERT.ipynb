{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8eedd85e",
   "metadata": {},
   "source": [
    "# Text Classification with BERT\n",
    "\n",
    "BERT and other Transformer encoder architectures have been wildly successful on a variety of tasks in NLP (natural language processing). They compute vector-space representations of natural language that are suitable for use in deep learning models. The BERT family of models uses the Transformer encoder architecture to process each token of input text in the full context of all tokens before and after, hence the name: Bidirectional Encoder Representations from Transformers.\n",
    "\n",
    "BERT models are usually pre-trained on a large corpus of text, then fine-tuned for specific tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e278eb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading required packages\n",
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "from official.nlp import optimization  # to create AdamW optimizer\n",
    "import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f522ed27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF Version: 2.7.0\n",
      "Keras Version: 2.7.0\n",
      "Number of available GPUs: 1\n"
     ]
    }
   ],
   "source": [
    "# check keras and TF version used\n",
    "print('TF Version:', tf.__version__)\n",
    "print('Keras Version:', keras.__version__)\n",
    "print('Number of available GPUs:', len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f48940",
   "metadata": {},
   "source": [
    "## Reading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14be332a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "business    245652\n",
       "general      86372\n",
       "finance      22386\n",
       "tech          8915\n",
       "science       5595\n",
       "Name: tags, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the data\n",
    "df = pd.read_csv('../data/interim/covid_articles_preprocessed.csv')\n",
    "\n",
    "## Merge Tags\n",
    "\n",
    "tag_map = {'consumer':'general',\n",
    "           'healthcare':'science',\n",
    "           'automotive':'business',\n",
    "           'environment':'science',\n",
    "           'construction':'business',\n",
    "           'ai':'tech'}\n",
    "\n",
    "df['tags'] = [(lambda tags: tag_map[tags] if tags in tag_map.keys() else tags)(tags)\n",
    "                          for tags in df['topic_area']]\n",
    "df.tags.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84e80b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.content.values\n",
    "y = df.tags.values\n",
    "\n",
    "enc = LabelEncoder()\n",
    "y = enc.fit_transform(y)\n",
    "enc_tags_mapping = dict(zip(enc.transform(enc.classes_), enc.classes_))\n",
    "\n",
    "## Split the data in train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=21)\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(X_train, y_train, test_size=0.2, random_state=21)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231f6b5d",
   "metadata": {},
   "source": [
    "## Encoding the raw text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723c521a",
   "metadata": {},
   "source": [
    "## Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2938bbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_classifier_model(tfhub_handle_preprocess, tfhub_handle_encoder):\n",
    "    text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
    "    preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')\n",
    "    encoder_inputs = preprocessing_layer(text_input)\n",
    "    encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')\n",
    "    outputs = encoder(encoder_inputs)\n",
    "    net = outputs['pooled_output']\n",
    "    net = tf.keras.layers.Dropout(0.1)(net)\n",
    "    net = tf.keras.layers.Dense(5, activation='softmax', name='classifier')(net)\n",
    "    return tf.keras.Model(text_input, net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c912de5",
   "metadata": {},
   "source": [
    "In this notebook I will use a version of small BERT. Small BERTs have the same general architecture but fewer and/or smaller Transformer blocks, which lets you explore tradeoffs between speed, size and quality.\n",
    "\n",
    "Text inputs need to be transformed to numeric token ids and arranged in several Tensors before being input to BERT. TensorFlow Hub provides a matching preprocessing model for each of the BERT models, which implements this transformation using TF ops from the `TF.text` library. It is not necessary to run pure Python code outside the TensorFlow model to preprocess text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "865657fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfhub_handle_preprocess = 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'\n",
    "tfhub_handle_encoder = 'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1'\n",
    "classifier_model = build_classifier_model(tfhub_handle_preprocess, tfhub_handle_encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566deca6",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "I just assembelled all pieces required in my BERT model including the preprocessing module, BERT encoder, data, and classifier. The next step is to train the model using the news dataset.\n",
    "\n",
    "I will use the `tf.keras.losses.SparseCategoricalCrossentropy` for multi-class classification. For For fine-tuning I will use Adam, the same optimizer that BERT was originally trained with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7044811",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(name='sparse_categorical_crossentropy')\n",
    "metrics = tf.metrics.SparseCategoricalAccuracy('accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc82dd57",
   "metadata": {},
   "source": [
    "For the learning rate (init_lr), I will use the same schedule as BERT pre-training: linear decay of a notional initial learning rate, prefixed with a linear warm-up phase over the first 10% of training steps (num_warmup_steps). In line with the BERT paper, the initial learning rate is smaller for fine-tuning (best of 5e-5, 3e-5, 2e-5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f61380c",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "steps_per_epoch = tf.data.experimental.cardinality(tf.data.Dataset.from_tensor_slices(X_train)).numpy()\n",
    "num_train_steps = steps_per_epoch * epochs\n",
    "num_warmup_steps = int(0.1*num_train_steps)\n",
    "\n",
    "init_lr = 3e-5\n",
    "optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
    "                                          num_train_steps=num_train_steps,\n",
    "                                          num_warmup_steps=num_warmup_steps,\n",
    "                                          optimizer_type='adamw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1f82af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_model.compile(optimizer=optimizer,\n",
    "                         loss=loss,\n",
    "                         metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d586f143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " text (InputLayer)              [(None,)]            0           []                               \n",
      "                                                                                                  \n",
      " preprocessing (KerasLayer)     {'input_word_ids':   0           ['text[0][0]']                   \n",
      "                                (None, 128),                                                      \n",
      "                                 'input_type_ids':                                                \n",
      "                                (None, 128),                                                      \n",
      "                                 'input_mask': (Non                                               \n",
      "                                e, 128)}                                                          \n",
      "                                                                                                  \n",
      " BERT_encoder (KerasLayer)      {'sequence_output':  28763649    ['preprocessing[0][0]',          \n",
      "                                 (None, 128, 512),                'preprocessing[0][1]',          \n",
      "                                 'default': (None,                'preprocessing[0][2]']          \n",
      "                                512),                                                             \n",
      "                                 'pooled_output': (                                               \n",
      "                                None, 512),                                                       \n",
      "                                 'encoder_outputs':                                               \n",
      "                                 [(None, 128, 512),                                               \n",
      "                                 (None, 128, 512),                                                \n",
      "                                 (None, 128, 512),                                                \n",
      "                                 (None, 128, 512)]}                                               \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 512)          0           ['BERT_encoder[0][5]']           \n",
      "                                                                                                  \n",
      " classifier (Dense)             (None, 5)            2565        ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 28,766,214\n",
      "Trainable params: 28,766,213\n",
      "Non-trainable params: 1\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "classifier_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3acf9a0",
   "metadata": {},
   "source": [
    "## Run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e2475d9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model with https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1\n",
      "Epoch 1/10\n",
      "6457/6457 [==============================] - 7023s 1s/step - loss: 0.8516 - accuracy: 0.6841 - val_loss: 0.5647 - val_accuracy: 0.7832\n",
      "Epoch 2/10\n",
      "6457/6457 [==============================] - 7007s 1s/step - loss: 0.5202 - accuracy: 0.7981 - val_loss: 0.4378 - val_accuracy: 0.8270\n",
      "Epoch 3/10\n",
      "6457/6457 [==============================] - 7003s 1s/step - loss: 0.4245 - accuracy: 0.8317 - val_loss: 0.3683 - val_accuracy: 0.8534\n",
      "Epoch 4/10\n",
      "6457/6457 [==============================] - 6997s 1s/step - loss: 0.3668 - accuracy: 0.8548 - val_loss: 0.3265 - val_accuracy: 0.8685\n",
      "Epoch 5/10\n",
      "6457/6457 [==============================] - 6989s 1s/step - loss: 0.3254 - accuracy: 0.8713 - val_loss: 0.3057 - val_accuracy: 0.8773\n",
      "Epoch 6/10\n",
      "6457/6457 [==============================] - 6994s 1s/step - loss: 0.2928 - accuracy: 0.8848 - val_loss: 0.2913 - val_accuracy: 0.8844\n",
      "Epoch 7/10\n",
      "6457/6457 [==============================] - 6994s 1s/step - loss: 0.2641 - accuracy: 0.8962 - val_loss: 0.2765 - val_accuracy: 0.8928\n",
      "Epoch 8/10\n",
      "6457/6457 [==============================] - 6994s 1s/step - loss: 0.2375 - accuracy: 0.9082 - val_loss: 0.2807 - val_accuracy: 0.8944\n"
     ]
    }
   ],
   "source": [
    "print(f'Training model with {tfhub_handle_encoder}')\n",
    "\n",
    "checkpoint_path = \"../models/DL/bert-train/cp.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "# Create a callback that saves the model's weights\n",
    "my_callbacks = [\n",
    "                keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=False),\n",
    "                keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                              patience=1,\n",
    "                                              restore_best_weights=False),\n",
    "                ]\n",
    "\n",
    "history = classifier_model.fit(X_train, y_train,\n",
    "                               epochs=epochs,\n",
    "                               verbose=True,\n",
    "                               validation_data=(X_validation, y_validation),\n",
    "                               callbacks=[my_callbacks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d2946331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    business       0.95      0.92      0.93     73935\n",
      "     finance       0.71      0.81      0.76      6788\n",
      "     general       0.83      0.89      0.86     25646\n",
      "     science       0.84      0.60      0.70      1690\n",
      "        tech       0.68      0.67      0.68      2617\n",
      "\n",
      "    accuracy                           0.89    110676\n",
      "   macro avg       0.80      0.78      0.79    110676\n",
      "weighted avg       0.90      0.89      0.89    110676\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_prob = classifier_model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "print(classification_report(y_test, y_pred, target_names=list(enc_tags_mapping.values())))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80abcf2",
   "metadata": {},
   "source": [
    "The results indicates that Small BERT provide a lower accuracy compared to the custom CNN model I created for this problem. I will save model for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "332dcf63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as restored_function_body, restored_function_body, restored_function_body, restored_function_body, restored_function_body while saving (showing 5 of 310). These functions will not be directly callable after loading.\n"
     ]
    }
   ],
   "source": [
    "classifier_model.save( \"../models/DL/bert-model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
